{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence to Sequence Learning with Neural Networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "import spacy\n",
    "import torch\n",
    "import torchtext\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EN</th>\n",
       "      <th>FR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Salut!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Cours !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Courez !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Who?</td>\n",
       "      <td>Qui ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wow!</td>\n",
       "      <td>Ça alors !</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     EN          FR\n",
       "0   Hi.      Salut!\n",
       "1  Run!     Cours !\n",
       "2  Run!    Courez !\n",
       "3  Who?       Qui ?\n",
       "4  Wow!  Ça alors !"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET_PATH = os.path.join(\n",
    "    \"..\", \n",
    "    \"..\", \n",
    "    \"nlp\", \n",
    "    \"datasets\", \n",
    "    \"en-fr-translation\", \n",
    "    \"en-fr.csv\"\n",
    ")\n",
    "\n",
    "df = pd.read_csv(DATASET_PATH)\n",
    "df = df.rename(columns={\"English words/sentences\": \"EN\"})\n",
    "df = df.rename(columns={\"French words/sentences\": \"FR\"})\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_tokenizer = get_tokenizer(\"spacy\", \"en_core_web_sm\")\n",
    "fr_tokenizer = get_tokenizer(\"spacy\", \"fr_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sentence(sentence: str):\n",
    "    pattern = r\"([.,!?:;]+)\"\n",
    "    sentence = re.sub(pattern, r\" \\1 \", sentence)\n",
    "\n",
    "    pattern = r\"\\s+\"\n",
    "    sentence = re.sub(pattern, \" \", sentence)\n",
    "\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def iterate_corpus(corpus: List[str], tokenizer: spacy.tokenizer.Tokenizer, max_len: int):\n",
    "    for sentence in corpus:\n",
    "        tokens = tokenizer(\n",
    "            prepare_sentence(sentence)\n",
    "        )\n",
    "\n",
    "        # Adding padding if it is needed.\n",
    "        if len(tokens) >= max_len:\n",
    "            tokens = tokens[:max_len]\n",
    "        else:\n",
    "            len_diff = max_len - len(tokens)\n",
    "            tokens = tokens + [\"<pad>\"] * len_diff\n",
    "\n",
    "        yield tokens\n",
    "\n",
    "\n",
    "en_corpus = [sent for sent in list(df[\"EN\"])]\n",
    "fr_corpus = [sent for sent in list(df[\"FR\"])]\n",
    "EN_MAX_LEN = 200\n",
    "FR_MAX_LEN = 200\n",
    "\n",
    "en_vocab = build_vocab_from_iterator(\n",
    "    iterate_corpus(en_corpus, en_tokenizer, EN_MAX_LEN), \n",
    "    specials=[\"<unk>\", \"<start>\", \"<end>\", \"<pad>\"]\n",
    ")\n",
    "en_vocab.set_default_index(en_vocab[\"<unk>\"])\n",
    "\n",
    "fr_vocab = build_vocab_from_iterator(\n",
    "    iterate_corpus(fr_corpus, fr_tokenizer, FR_MAX_LEN), \n",
    "    specials=[\"<unk>\", \"<start>\", \"<end>\", \"<pad>\"]\n",
    ")\n",
    "fr_vocab.set_default_index(fr_vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: torch.Size([175621, 200])\n",
      "y.shape: torch.Size([175621, 200])\n"
     ]
    }
   ],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        lang1_corpus: List[str], lang2_corpus: List[str],\n",
    "        lang1_tokenizer: spacy.tokenizer.Tokenizer, lang2_tokenizer: spacy.tokenizer.Tokenizer,\n",
    "        lang1_vocab: torchtext.vocab.Vocab, lang2_vocab: torchtext.vocab.Vocab,\n",
    "        lang1_max_len: int = 200, lang2_max_len: int = 200\n",
    "    ):\n",
    "        self.l1_corpus = lang1_corpus\n",
    "        self.l2_corpus = lang2_corpus\n",
    "\n",
    "        self.l1_max_len = lang1_max_len\n",
    "        self.l2_max_len = lang2_max_len\n",
    "        \n",
    "        self.l1_tokenizer = lang1_tokenizer\n",
    "        self.l2_tokenizer = lang2_tokenizer\n",
    "        \n",
    "        self.l1_vocab = lang1_vocab\n",
    "        self.l2_vocab = lang2_vocab\n",
    "\n",
    "        self.x, self.y = self._get_x_y()\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def _get_x_y(self):\n",
    "        x = TranslationDataset._parse_corpus(\n",
    "            self.l1_corpus, self.l1_tokenizer, self.l1_vocab, self.l1_max_len\n",
    "        )\n",
    "        y = TranslationDataset._parse_corpus(\n",
    "            self.l2_corpus, self.l2_tokenizer, self.l2_vocab, self.l2_max_len\n",
    "        )\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    @staticmethod\n",
    "    def _parse_corpus(\n",
    "        corpus: List[str], \n",
    "        tokenizer: spacy.tokenizer.Tokenizer, \n",
    "        vocab: torchtext.vocab.Vocab,\n",
    "        max_len: int\n",
    "    ):\n",
    "        output = []\n",
    "\n",
    "        for sent in corpus:\n",
    "            tokens = tokenizer(sent)\n",
    "            indices = [vocab[token] for token in tokens]\n",
    "            if len(indices) >= max_len:\n",
    "                output.append(indices[:max_len])\n",
    "            else:\n",
    "                len_diff = max_len - len(indices)\n",
    "                padding = [vocab[\"<pad>\"]] * len_diff\n",
    "                output.append(indices + padding)\n",
    "\n",
    "        return torch.LongTensor(output)\n",
    "\n",
    "dataset = TranslationDataset(\n",
    "    lang1_corpus=en_corpus, lang2_corpus=fr_corpus,\n",
    "    lang1_vocab=en_vocab, lang2_vocab=fr_vocab,\n",
    "    lang1_tokenizer=en_tokenizer, lang2_tokenizer=fr_tokenizer,\n",
    "    lang1_max_len=EN_MAX_LEN, lang2_max_len=FR_MAX_LEN\n",
    ")\n",
    "print(\"x.shape:\", dataset.x.shape)\n",
    "print(\"y.shape:\", dataset.y.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self):\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8100bb27ef6f27bb6b63ba202e13f32f0dffed430e6a4d162d3986e448f218b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
