{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence to Sequence Learning with Neural Networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "import spacy\n",
    "import torch\n",
    "import torchtext\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.utils.data import random_split\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "\n",
    "# Controlling the randomness in PyTorch\n",
    "RANDOM_SEED = 0\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Dataset\n",
    "I've used a Kaggle dataset. You can download it from [here](https://www.kaggle.com/datasets/devicharith/language-translation-englishfrench)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EN</th>\n",
       "      <th>FR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Salut!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Cours !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Courez !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Who?</td>\n",
       "      <td>Qui ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wow!</td>\n",
       "      <td>Ça alors !</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     EN          FR\n",
       "0   Hi.      Salut!\n",
       "1  Run!     Cours !\n",
       "2  Run!    Courez !\n",
       "3  Who?       Qui ?\n",
       "4  Wow!  Ça alors !"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET_PATH = os.path.join(\n",
    "    \"..\", \n",
    "    \"..\", \n",
    "    \"nlp\", \n",
    "    \"datasets\", \n",
    "    \"en-fr-translation\", \n",
    "    \"en-fr.csv\"\n",
    ")\n",
    "\n",
    "df = pd.read_csv(DATASET_PATH)\n",
    "df = df.rename(columns={\"English words/sentences\": \"EN\"})\n",
    "df = df.rename(columns={\"French words/sentences\": \"FR\"})\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sentence(sentence: str):\n",
    "    pattern = r\"([.,!?:;]+)\"\n",
    "    sentence = re.sub(pattern, r\" \\1 \", sentence)\n",
    "\n",
    "    pattern = r\"\\s+\"\n",
    "    sentence = re.sub(pattern, \" \", sentence)\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<AxesSubplot:title={'center':'EN word count'}>,\n",
       "        <AxesSubplot:title={'center':'FR word count'}>]], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[\"EN word count\"] = df[\"EN\"].apply(lambda x: len(prepare_sentence(x).split(\" \")))\n",
    "df[\"FR word count\"] = df[\"FR\"].apply(lambda x: len(prepare_sentence(x).split(\" \")))\n",
    "df.hist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the distributions, in both languages it would be okay to select $20$ as the max size of an input/output sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffling and generating a subsample of the dataframe.\n",
    "DATASET_FRACTION = 0.5\n",
    "df = df.sample(frac=DATASET_FRACTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You should first download these two spaCy models!\n",
    "en_tokenizer = get_tokenizer(\"spacy\", \"en_core_web_sm\")\n",
    "fr_tokenizer = get_tokenizer(\"spacy\", \"fr_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_corpus(corpus: List[str], tokenizer: spacy.tokenizer.Tokenizer, max_len: int):\n",
    "    for sentence in corpus:\n",
    "        tokens = tokenizer(\n",
    "            prepare_sentence(sentence)\n",
    "        )\n",
    "\n",
    "        # Adding padding if it is needed.\n",
    "        if len(tokens) >= max_len:\n",
    "            tokens = tokens[:max_len]\n",
    "        else:\n",
    "            len_diff = max_len - len(tokens)\n",
    "            tokens = tokens + [\"<pad>\"] * len_diff\n",
    "\n",
    "        yield tokens\n",
    "\n",
    "\n",
    "en_corpus = [sent for sent in list(df[\"EN\"])]\n",
    "fr_corpus = [sent for sent in list(df[\"FR\"])]\n",
    "EN_MAX_LEN = 20\n",
    "FR_MAX_LEN = 20\n",
    "SPECIALS = [\"<unk>\", \"<start>\", \"<end>\", \"<pad>\"]\n",
    "\n",
    "en_vocab = build_vocab_from_iterator(\n",
    "    iterate_corpus(en_corpus, en_tokenizer, EN_MAX_LEN), \n",
    "    specials=SPECIALS\n",
    ")\n",
    "en_vocab.set_default_index(en_vocab[\"<unk>\"])\n",
    "\n",
    "fr_vocab = build_vocab_from_iterator(\n",
    "    iterate_corpus(fr_corpus, fr_tokenizer, FR_MAX_LEN), \n",
    "    specials=SPECIALS\n",
    ")\n",
    "fr_vocab.set_default_index(fr_vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the 'l1_idx2token' attribute:\n",
      "['I', \"'m\", 'sure', 'there', \"'s\", 'another', 'way', 'out', '.', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "x.shape: torch.Size([87810, 20])\n",
      "y.shape: torch.Size([87810, 20])\n"
     ]
    }
   ],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        lang1_corpus: List[str], lang2_corpus: List[str],\n",
    "        lang1_tokenizer: spacy.tokenizer.Tokenizer, lang2_tokenizer: spacy.tokenizer.Tokenizer,\n",
    "        lang1_vocab: torchtext.vocab.Vocab, lang2_vocab: torchtext.vocab.Vocab,\n",
    "        lang1_max_len: int = 200, lang2_max_len: int = 200, special_tokens=SPECIALS\n",
    "    ):\n",
    "        self.l1_corpus = lang1_corpus\n",
    "        self.l2_corpus = lang2_corpus\n",
    "\n",
    "        self.l1_max_len = lang1_max_len\n",
    "        self.l2_max_len = lang2_max_len\n",
    "        \n",
    "        self.l1_tokenizer = lang1_tokenizer\n",
    "        self.l2_tokenizer = lang2_tokenizer\n",
    "        \n",
    "        self.l1_vocab = lang1_vocab\n",
    "        self.l2_vocab = lang2_vocab\n",
    "\n",
    "        self.l1_token2idx = lang1_vocab\n",
    "        self.l1_idx2token = {\n",
    "            lang1_vocab[word]: word\n",
    "            for sentence in lang1_corpus\n",
    "            for word in lang1_tokenizer(prepare_sentence(sentence)) + SPECIALS\n",
    "        }\n",
    "        self.l2_token2idx = lang2_vocab\n",
    "        self.l2_idx2token = {\n",
    "            lang2_vocab[word]: word\n",
    "            for sentence in lang2_corpus\n",
    "            for word in lang2_tokenizer(prepare_sentence(sentence)) + SPECIALS\n",
    "        }\n",
    "\n",
    "        self.x, self.y = self._get_x_y()\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def _get_x_y(self):\n",
    "        x = TranslationDataset._parse_corpus(\n",
    "            self.l1_corpus, self.l1_tokenizer, self.l1_vocab, self.l1_max_len\n",
    "        )\n",
    "        y = TranslationDataset._parse_corpus(\n",
    "            self.l2_corpus, self.l2_tokenizer, self.l2_vocab, self.l2_max_len\n",
    "        )\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    @staticmethod\n",
    "    def _parse_corpus(\n",
    "        corpus: List[str], \n",
    "        tokenizer: spacy.tokenizer.Tokenizer, \n",
    "        vocab: torchtext.vocab.Vocab,\n",
    "        max_len: int\n",
    "    ):\n",
    "        output = []\n",
    "\n",
    "        for sent in corpus:\n",
    "            tokens = tokenizer(sent)\n",
    "            indices = [vocab[token] for token in tokens]\n",
    "            if len(indices) >= max_len:\n",
    "                output.append(indices[:max_len])\n",
    "            else:\n",
    "                len_diff = max_len - len(indices)\n",
    "                padding = [vocab[\"<pad>\"]] * len_diff\n",
    "                output.append(indices + padding)\n",
    "\n",
    "        return torch.LongTensor(output)\n",
    "\n",
    "\n",
    "dataset = TranslationDataset(\n",
    "    lang1_corpus=en_corpus, lang2_corpus=fr_corpus,\n",
    "    lang1_vocab=en_vocab, lang2_vocab=fr_vocab,\n",
    "    lang1_tokenizer=en_tokenizer, lang2_tokenizer=fr_tokenizer,\n",
    "    lang1_max_len=EN_MAX_LEN, lang2_max_len=FR_MAX_LEN\n",
    ")\n",
    "# Printing an example sentence, using the l1_idx2token dictionary.\n",
    "# l1 here means language 1, which is English.\n",
    "x_sample = dataset.x[1]\n",
    "print(\"Using the 'l1_idx2token' attribute:\")\n",
    "print([dataset.l1_idx2token[int(idx)] for idx in x_sample])\n",
    "\n",
    "print(\"x.shape:\", dataset.x.shape)\n",
    "print(\"y.shape:\", dataset.y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 70248\n",
      "Validation dataset size: 17562\n"
     ]
    }
   ],
   "source": [
    "def train_validation_split(dataset: torch.utils.data.Dataset, train_size: float):\n",
    "    train_set_size = int(len(dataset) * train_size)\n",
    "    valid_set_size = len(dataset) - train_set_size\n",
    "    datasets_lengths = [train_set_size, valid_set_size]\n",
    "\n",
    "    # Splitting the input dataset into training and validation set.\n",
    "    train_dataset, valid_dataset = random_split(dataset, datasets_lengths)\n",
    "\n",
    "    return train_dataset, valid_dataset\n",
    "\n",
    "\n",
    "train_dataset, valid_dataset = train_validation_split(\n",
    "    dataset, train_size=0.8\n",
    ")\n",
    "print(\"Training dataset size:\", len(train_dataset))\n",
    "print(\"Validation dataset size:\", len(valid_dataset))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Seq2Seq Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: torch.Size([10, 5, 100])\n"
     ]
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size: int, embed_size: int, hidden_size: int, num_layers: int, padding_idx: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size, padding_idx=padding_idx)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embed_size, \n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, hidden: tuple):\n",
    "        embedded_x = self.embed(x)\n",
    "        hidden = [state.detach() for state in hidden]\n",
    "        out, hidden = self.lstm(embedded_x, hidden)\n",
    "\n",
    "        return out, hidden\n",
    "\n",
    "    def get_init_hidden(self, batch_size):\n",
    "        # See the PyTorch documentation of LSTM for these dimensions.\n",
    "        h = torch.zeros(size=(self.num_layers, batch_size, self.hidden_size))\n",
    "        c = torch.zeros(size=(self.num_layers, batch_size, self.hidden_size))\n",
    "\n",
    "        return (h, c)\n",
    "\n",
    "\n",
    "x = torch.randint(low=0, high=20, size=(10, 5))\n",
    "enc = Encoder(vocab_size=50, embed_size=100, hidden_size=100, num_layers=4, padding_idx=0)\n",
    "h, c = enc.get_init_hidden(10)\n",
    "out, hidden = enc(x, (h, c))\n",
    "print(f\"Encoder output shape: {out.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: torch.Size([10, 5, 150])\n",
      "Decoder output shape: torch.Size([10, 5, 200])\n"
     ]
    }
   ],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size: int, embed_size: int, hidden_size: int, num_layers: int, padding_idx: int):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size, padding_idx=padding_idx)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embed_size, \n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.project = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, hidden: tuple):\n",
    "        embedded_x = self.embed(x)\n",
    "        hidden = [state.detach() for state in hidden]\n",
    "        out, hidden = self.lstm(embedded_x, hidden)\n",
    "        out = self.project(out)\n",
    "\n",
    "        return out, hidden\n",
    "\n",
    "\n",
    "# Passing through the Encoder\n",
    "x = torch.randint(low=0, high=20, size=(10, 5))\n",
    "enc = Encoder(vocab_size=50, embed_size=100, hidden_size=150, num_layers=4, padding_idx=0)\n",
    "hidden = enc.get_init_hidden(10)\n",
    "out, hidden = enc(x, hidden)\n",
    "print(f\"Encoder output shape: {out.shape}\")\n",
    "y = torch.randint(low=0, high=40, size=(10, 5))\n",
    "\n",
    "# Passing through the Decoder\n",
    "dec = Decoder(vocab_size=200, embed_size=120, hidden_size=150, num_layers=4, padding_idx=0)\n",
    "out, hidden = dec(y, hidden)\n",
    "print(f\"Decoder output shape: {out.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embed): Embedding(50, 100, padding_idx=0)\n",
      "    (lstm): LSTM(100, 150, num_layers=4, batch_first=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embed): Embedding(100, 100, padding_idx=0)\n",
      "    (lstm): LSTM(100, 150, num_layers=4, batch_first=True)\n",
      "    (project): Linear(in_features=150, out_features=100, bias=True)\n",
      "  )\n",
      ")\n",
      "x shape: torch.Size([10, 5])\n",
      "y shape: torch.Size([10, 5])\n",
      "Seq2Seq output shape: torch.Size([10, 5, 100])\n",
      "Seq2Seq hidden shape: torch.Size([4, 10, 150])\n"
     ]
    }
   ],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder: torch.nn.Module, decoder: torch.nn.Module, reverse_input=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "        self.reverse_input = reverse_input\n",
    "\n",
    "    def forward(self, x: torch.Tensor, y: torch.Tensor, hidden: tuple):\n",
    "        # Reversing the sequences in the input tensor.\n",
    "        # In the paper it's stated that it is extremely valuable, and makes a difference.\n",
    "        if self.reverse_input:\n",
    "            x = torch.flip(x, [1])\n",
    "        _, hidden = self.encoder(x, hidden)\n",
    "        out, hidden = self.decoder(y, hidden)\n",
    "\n",
    "        return out, hidden\n",
    "\n",
    "\n",
    "# Input tensor:\n",
    "x = torch.randint(low=0, high=20, size=(10, 5))\n",
    "y = torch.randint(low=0, high=40, size=(10, 5))\n",
    "\n",
    "# Defining the Encoder:\n",
    "enc = Encoder(vocab_size=50, embed_size=100, hidden_size=150, num_layers=4, padding_idx=0)\n",
    "init_hidden = enc.get_init_hidden(10)\n",
    "\n",
    "# Defining the decoder:\n",
    "dec = Decoder(vocab_size=100, embed_size=100, hidden_size=150, num_layers=4, padding_idx=0)\n",
    "\n",
    "# Creating the Seq2Seq model:\n",
    "model = Seq2Seq(encoder=enc, decoder=dec)\n",
    "out, hidden = model(x, y, init_hidden)\n",
    "\n",
    "print(f\"Model:\\n{model}\")\n",
    "\n",
    "print(f\"x shape: {x.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "\n",
    "print(f\"Seq2Seq output shape: {out.shape}\")\n",
    "print(f\"Seq2Seq hidden shape: {hidden[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(model: torch.nn.Module):\n",
    "    \"\"\"Initializing the weights with the uniform distribution between -0.08 and \n",
    "    0.08.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model for which the weights will be initialized.\n",
    "    \"\"\"\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"bias\" in name or \"weight\" in name:\n",
    "            torch.nn.init.uniform_(param, a=-0.08, b=0.08)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Training Batch 0, Loss: 9.97\n",
      "Bleu Score: 0.00\n",
      "Example\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Target: Saisis un balai et aide -nous à nettoyer ! <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Prediction: attendre séjournait séjournait séjournait séjournait séjournait séjournait séjournait séjournait séjournait séjournait séjournait séjournait séjournait séjournait séjournait séjournait séjournait séjournait séjournait\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training Batch 150, Loss: 3.22\n",
      "Bleu Score: 0.00\n",
      "Example\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Target: Ils nous détestent . <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Prediction: Je est . . . . . . <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training Batch 300, Loss: 2.58\n",
      "Bleu Score: 0.00\n",
      "Example\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Target: Les prix augmentent après six heures . <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Prediction: Je Je est pas . . . <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training Batch 450, Loss: 2.64\n",
      "Bleu Score: 0.00\n",
      "Example\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Target: Passez -moi la confiture . <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Prediction: Je est est de . <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training Batch 600, Loss: 2.52\n",
      "Bleu Score: 0.00\n",
      "Example\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Target: Je n' ai réellement besoin que de me trouver seule un moment . <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Prediction: Je ne que pas pas pas pas pas de de de de . <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\machine-learning\\dl-papers\\notebooks\\Sequence to Sequence Learning with Neural Networks.ipynb Cell 23\u001b[0m in \u001b[0;36m<cell line: 206>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/machine-learning/dl-papers/notebooks/Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb#X31sZmlsZQ%3D%3D?line=195'>196</a>\u001b[0m training_session \u001b[39m=\u001b[39m Seq2SeqTrainingSession(\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/machine-learning/dl-papers/notebooks/Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb#X31sZmlsZQ%3D%3D?line=196'>197</a>\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/machine-learning/dl-papers/notebooks/Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb#X31sZmlsZQ%3D%3D?line=197'>198</a>\u001b[0m     loss\u001b[39m=\u001b[39mloss,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/machine-learning/dl-papers/notebooks/Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb#X31sZmlsZQ%3D%3D?line=202'>203</a>\u001b[0m     device\u001b[39m=\u001b[39mDEVICE\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/machine-learning/dl-papers/notebooks/Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb#X31sZmlsZQ%3D%3D?line=203'>204</a>\u001b[0m )\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/machine-learning/dl-papers/notebooks/Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb#X31sZmlsZQ%3D%3D?line=204'>205</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDevice: \u001b[39m\u001b[39m{\u001b[39;00mDEVICE\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/machine-learning/dl-papers/notebooks/Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb#X31sZmlsZQ%3D%3D?line=205'>206</a>\u001b[0m training_session\u001b[39m.\u001b[39;49mstart(\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/machine-learning/dl-papers/notebooks/Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb#X31sZmlsZQ%3D%3D?line=206'>207</a>\u001b[0m     train_dataset\u001b[39m=\u001b[39;49mtrain_dataset, \n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/machine-learning/dl-papers/notebooks/Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb#X31sZmlsZQ%3D%3D?line=207'>208</a>\u001b[0m     valid_dataset\u001b[39m=\u001b[39;49mvalid_dataset, \n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/machine-learning/dl-papers/notebooks/Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb#X31sZmlsZQ%3D%3D?line=208'>209</a>\u001b[0m     initial_hidden\u001b[39m=\u001b[39;49minit_hidden,\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/machine-learning/dl-papers/notebooks/Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb#X31sZmlsZQ%3D%3D?line=209'>210</a>\u001b[0m     l1_idx2token\u001b[39m=\u001b[39;49mdataset\u001b[39m.\u001b[39;49ml1_idx2token,\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/machine-learning/dl-papers/notebooks/Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb#X31sZmlsZQ%3D%3D?line=210'>211</a>\u001b[0m     l2_idx2token\u001b[39m=\u001b[39;49mdataset\u001b[39m.\u001b[39;49ml2_idx2token\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/machine-learning/dl-papers/notebooks/Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb#X31sZmlsZQ%3D%3D?line=211'>212</a>\u001b[0m )\n",
      "\u001b[1;32md:\\machine-learning\\dl-papers\\notebooks\\Sequence to Sequence Learning with Neural Networks.ipynb Cell 23\u001b[0m in \u001b[0;36mSeq2SeqTrainingSession.start\u001b[1;34m(self, train_dataset, valid_dataset, initial_hidden, l1_idx2token, l2_idx2token, fixed_sentences)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/machine-learning/dl-papers/notebooks/Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb#X31sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m valid_dataloader \u001b[39m=\u001b[39m DataLoader(\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/machine-learning/dl-papers/notebooks/Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb#X31sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     train_dataset, \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/machine-learning/dl-papers/notebooks/Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb#X31sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/machine-learning/dl-papers/notebooks/Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb#X31sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     drop_last\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/machine-learning/dl-papers/notebooks/Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb#X31sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/machine-learning/dl-papers/notebooks/Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb#X31sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepochs):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/machine-learning/dl-papers/notebooks/Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb#X31sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m     train_loss, train_bleu \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_epoch(train_dataloader)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/machine-learning/dl-papers/notebooks/Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb#X31sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m     valid_loss, valid_bleu \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_valid_epoch(valid_dataloader)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/machine-learning/dl-papers/notebooks/Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb#X31sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Training Loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Train BLEU: \u001b[39m\u001b[39m{\u001b[39;00mtrain_bleu\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Validation Loss: \u001b[39m\u001b[39m{\u001b[39;00mvalid_loss\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Validation BLEU: \u001b[39m\u001b[39m{\u001b[39;00mvalid_bleu\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32md:\\machine-learning\\dl-papers\\notebooks\\Sequence to Sequence Learning with Neural Networks.ipynb Cell 23\u001b[0m in \u001b[0;36mSeq2SeqTrainingSession._train_epoch\u001b[1;34m(self, dataloader)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/machine-learning/dl-papers/notebooks/Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb#X31sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minit_hidden\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/machine-learning/dl-papers/notebooks/Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb#X31sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch_i, (x, y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/machine-learning/dl-papers/notebooks/Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb#X31sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m     x, y \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39;49mto(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice), y\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/machine-learning/dl-papers/notebooks/Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb#X31sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m     y_pred, hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(x, y, hidden)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/machine-learning/dl-papers/notebooks/Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb#X31sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m     \u001b[39m# Here y_pred should be trasformed to shape (BATCH_SIZE * SEQ_LEN, VOCAB_SIZE)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/machine-learning/dl-papers/notebooks/Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb#X31sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m     \u001b[39m# and y should be transformed to shape (BATCH_SIZE * SEQ_LEN).\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class Seq2SeqTrainingSession:\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        model: torch.nn.Module, \n",
    "        loss: torch.nn.Module, \n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        epochs: int, batch_size: int, \n",
    "        use_clipping=True,\n",
    "        device=\"cpu\"\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.loss_func = loss\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.use_clipping = use_clipping\n",
    "        self.clip = 5\n",
    "        self.device = device\n",
    "\n",
    "    def start(\n",
    "        self, \n",
    "        train_dataset: torch.utils.data.Subset, \n",
    "        valid_dataset: torch.utils.data.Subset,\n",
    "        initial_hidden: torch.Tensor,\n",
    "        l1_idx2token: dict,\n",
    "        l2_idx2token: dict,\n",
    "        fixed_sentences: List[List[str]] = None\n",
    "    ):\n",
    "        self.init_hidden = initial_hidden\n",
    "        self.fixed_sentences = fixed_sentences\n",
    "\n",
    "        self.l1_idx2token = l1_idx2token\n",
    "        self.l2_idx2token = l2_idx2token\n",
    "        \n",
    "        train_dataloader = DataLoader(\n",
    "            train_dataset, \n",
    "            self.batch_size, \n",
    "            shuffle=True, \n",
    "            num_workers=0,\n",
    "            drop_last=True\n",
    "        )\n",
    "        valid_dataloader = DataLoader(\n",
    "            train_dataset, \n",
    "            self.batch_size, \n",
    "            shuffle=True, \n",
    "            num_workers=0,\n",
    "            drop_last=True\n",
    "        )\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            train_loss, train_bleu = self._train_epoch(train_dataloader)\n",
    "            valid_loss, valid_bleu = self._valid_epoch(valid_dataloader)\n",
    "            print(f\"Epoch: {epoch + 1}, Training Loss: {train_loss:.2f}, Train BLEU: {train_bleu:.2f}, Validation Loss: {valid_loss:.2f}, Validation BLEU: {valid_bleu:.2f}\")\n",
    "\n",
    "        return self.model\n",
    "\n",
    "    def _train_epoch(self, dataloader):\n",
    "        hidden = self.init_hidden\n",
    "\n",
    "        for batch_i, (x, y) in enumerate(dataloader):\n",
    "            x, y = x.to(self.device), y.to(self.device)\n",
    "\n",
    "            y_pred, hidden = self.model(x, y, hidden)\n",
    "\n",
    "            # Here y_pred should be trasformed to shape (BATCH_SIZE * SEQ_LEN, VOCAB_SIZE)\n",
    "            # and y should be transformed to shape (BATCH_SIZE * SEQ_LEN).\n",
    "            loss = self.loss_func(y_pred.reshape((-1, y_pred.shape[-1])), y.reshape(-1))\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            # Clipping the gradients, since LSTMs can have exploding gradients.\n",
    "            if self.use_clipping:\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    self.model.parameters(), \n",
    "                    max_norm=self.clip\n",
    "                )\n",
    "\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if batch_i % 150 == 0:\n",
    "                pred_sents, target_sents = self._tensors2sentences(y_pred[:1], y[:1])\n",
    "                print(f\"Training Batch {batch_i}, Loss: {loss.item():.2f}\")\n",
    "                bleu = self._calc_bleu(y_pred, y)\n",
    "                print(f\"Bleu Score: {bleu:.2f}\")\n",
    "                print(\"Example\")\n",
    "                print(\"-\" * 100)\n",
    "                print(f\"Target: {' '.join(target_sents)}\")\n",
    "                print(f\"Prediction: {' '.join(pred_sents)}\")\n",
    "                print(\"-\" * 100)\n",
    "\n",
    "        epoch_loss = loss.item()\n",
    "        epoch_bleu = self._calc_bleu(y_pred, y)\n",
    "\n",
    "        return epoch_loss, epoch_bleu\n",
    "\n",
    "    def _valid_epoch(self, dataloader):\n",
    "        hidden = self.init_hidden\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_i, (x, y) in enumerate(dataloader):\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "            \n",
    "                y_pred, hidden = self.model(x, y, hidden)\n",
    "\n",
    "                loss = self.loss_func(y_pred.reshape((-1, y_pred.shape[-1])), y.reshape(-1))\n",
    "\n",
    "            epoch_loss = loss.item()\n",
    "            epoch_bleu = self._calc_bleu(y_pred, y)\n",
    "\n",
    "        self.model.train()\n",
    "        \n",
    "        return epoch_loss, epoch_bleu\n",
    "\n",
    "    def _tensors2sentences(self, y_pred: torch.Tensor, y):\n",
    "        predictions = y_pred.argmax(-1).tolist()\n",
    "        targets = y.tolist()\n",
    "        predictions = [\n",
    "            self.l2_idx2token[idx] \n",
    "            for sent in predictions \n",
    "            for idx in sent\n",
    "        ]\n",
    "        targets = [\n",
    "            self.l2_idx2token[idx] \n",
    "            for sent in targets \n",
    "            for idx in sent\n",
    "        ]\n",
    "\n",
    "        return predictions, targets\n",
    "\n",
    "    # TODO: Implement this\n",
    "    def _calc_bleu(self, y_pred, y):\n",
    "        predictions = y_pred.argmax(-1).tolist()\n",
    "        targets = y.tolist()\n",
    "\n",
    "        prediction_words = []\n",
    "        target_words = []\n",
    "        for i, (pred_sent, targ_sent) in enumerate(zip(predictions, targets)):\n",
    "            prediction_words.append([])\n",
    "            target_words.append([])\n",
    "            for pred_idx, targ_idx in zip(pred_sent, targ_sent):\n",
    "                prediction_words[i].append(self.l2_idx2token[pred_idx])\n",
    "                target_words[i].append(self.l2_idx2token[targ_idx])\n",
    "\n",
    "        return bleu_score(prediction_words, target_words)\n",
    "\n",
    "\n",
    "# The paper uses 1000, but their dataset is huge.\n",
    "EMBED_SIZE = 300\n",
    "HIDDEN_SIZE = 512\n",
    "NUM_LAYERS = 4\n",
    "# The batch size aws 128 in the paper, but my GPU has 2GB VRAM :D.\n",
    "BATCH_SIZE = 64\n",
    "L_RATE = 7e-1\n",
    "# The epochs in the paper are 7.5 (7 epochs and a half).\n",
    "# The model and the dataset are somewhat different, so I'll use different\n",
    "# number of epochs.\n",
    "EPOCHS = 5\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "enc = Encoder(\n",
    "    vocab_size=len(en_vocab),\n",
    "    embed_size=EMBED_SIZE,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    padding_idx=en_vocab[\"<pad>\"]\n",
    ")\n",
    "init_hidden = enc.get_init_hidden(batch_size=BATCH_SIZE)\n",
    "init_hidden = [h.to(DEVICE) for h in init_hidden]\n",
    "\n",
    "dec = Decoder(\n",
    "    vocab_size=len(fr_vocab),\n",
    "    embed_size=EMBED_SIZE,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    padding_idx=fr_vocab[\"<pad>\"]\n",
    ")\n",
    "\n",
    "model = Seq2Seq(\n",
    "    encoder=enc, decoder=dec, \n",
    "    reverse_input=True\n",
    ").to(DEVICE)\n",
    "\n",
    "model.apply(initialize_weights)\n",
    "\n",
    "# The loss function and the optimizer.\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=L_RATE)\n",
    "\n",
    "training_session = Seq2SeqTrainingSession(\n",
    "    model=model,\n",
    "    loss=loss,\n",
    "    optimizer=optimizer,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    use_clipping=True,\n",
    "    device=DEVICE\n",
    ")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "training_session.start(\n",
    "    train_dataset=train_dataset, \n",
    "    valid_dataset=valid_dataset, \n",
    "    initial_hidden=init_hidden,\n",
    "    l1_idx2token=dataset.l1_idx2token,\n",
    "    l2_idx2token=dataset.l2_idx2token\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8100bb27ef6f27bb6b63ba202e13f32f0dffed430e6a4d162d3986e448f218b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
