{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence to Sequence Learning with Neural Networks\n",
    "___\n",
    "\n",
    "This is a paper implementation in which I've tried to create the described model as closely as possible.\n",
    "You may find some differences in the hyperparameters but that's mainly because I'm training the model on a small machine with $2$ GB VRAM.\n",
    "\n",
    "I've tried to concisely describe the model architecture in section **Seq2Seq Model**, using quotes from the paper.\n",
    "\n",
    "The model was trained for $5$ epochs with final validation loss of approx. $1.00$, but it isn't shown in this notebook, because of printing limitations. There is still one thing that has to be implemented and that's the calculation of *BLEU* score.\n",
    "\n",
    "<small>Note: *Some special strings for start of sentence, end of sentence and padding do not appear on the Notebook after it's uploaded to Github - probably because the '\\<' and '\\>' symbols must be escaped*. Instead of them you'll see empty strings.</small>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "import spacy\n",
    "import torch\n",
    "import torchtext\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.utils.data import random_split\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "\n",
    "# Controlling the randomness in PyTorch and NumPy.\n",
    "RANDOM_SEED = 0\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Dataset\n",
    "I've used a Kaggle dataset. You can download it from [here](https://www.kaggle.com/datasets/devicharith/language-translation-englishfrench)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EN</th>\n",
       "      <th>FR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Salut!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Cours !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Courez !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Who?</td>\n",
       "      <td>Qui ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wow!</td>\n",
       "      <td>Ça alors !</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     EN          FR\n",
       "0   Hi.      Salut!\n",
       "1  Run!     Cours !\n",
       "2  Run!    Courez !\n",
       "3  Who?       Qui ?\n",
       "4  Wow!  Ça alors !"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET_PATH = os.path.join(\n",
    "    \"..\", \n",
    "    \"..\", \n",
    "    \"nlp\", \n",
    "    \"datasets\", \n",
    "    \"en-fr-translation\", \n",
    "    \"en-fr.csv\"\n",
    ")\n",
    "\n",
    "df = pd.read_csv(DATASET_PATH)\n",
    "df = df.rename(columns={\"English words/sentences\": \"EN\"})\n",
    "df = df.rename(columns={\"French words/sentences\": \"FR\"})\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sentence(sentence: str):\n",
    "    pattern = r\"([.,!?:;]+)\"\n",
    "    sentence = re.sub(pattern, r\" \\1 \", sentence)\n",
    "\n",
    "    pattern = r\"\\s+\"\n",
    "    sentence = re.sub(pattern, \" \", sentence)\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<AxesSubplot:title={'center':'EN word count'}>,\n",
       "        <AxesSubplot:title={'center':'FR word count'}>]], dtype=object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEICAYAAAC9E5gJAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAaxElEQVR4nO3df5BlZX3n8fdHRgJq5GdqAjMkM1knyRJNok4A16yZAqMDmgxVa1wMCYPLOlUrJpglq5itLIk/dnU3K0pWrSJCBOMGyMTIRDCERRtjKSgokQxIMQHMzDiAMvzIoKKj3/3jPBMv3be7b/d09+07/X5V3Zp7nvOcc57T83R/zj33OeekqpAkLW1PG3YDJEnDZxhIkgwDSZJhIEnCMJAkYRhIkjAMlowkf5Dkz4bdDmmhJLk/yUuH3Y5RYRjMUOtg30qyp+f1f9q8s5NUkjeNW2ZHknXDaO9ilWRdkh3DbocGM0m/PzbJqtbn95Xdn+SCYbd3MUryoSRvH3Y7JmMYzM6vVNWzel5v6Jm3G3hTkh8eVuOSLBvWtnVAG9/vv9Yz7/CqehbwKuD3k/zyQjXK/j43DIO5dxfwOeA/T1cxyeokjyZ5Wpv+kyQP9cz/cJI3tvfHJtmSZHeSbUle11PvD5JsTvJnSR4Hzm7rvinJPye5ATh6mrZsSHJ7kseT/GOS9QNs9ylHOuOP9ttR4u8m+XKSx5JcleSQJM8EPgEc23uUOd3PS4tfVd0KbAV+vt/8JH+Y5I/b+6cneSLJ/2rThyb5dpIj2/SvJtnafkfGkvzrnvXcn+TNSb4MPJFkWZLfTPLVJA8n+a9TtbNt63+3+o8l+UySQwfYbiV5Ts/0v/wO7Ov/Sc5P8lCSXUle2+ZtAs6kO1Dck+SvZ/7TnV+Gwfz4feCN+zr1ZKrqPuBx4Pmt6CXAnp7O90vATe39lcAO4Fi6o6//nuTkntVtADYDhwMfAf4vcBtdCLwN2DhZO5KcAFwB/Je2/EuA+wfc7nReDawHVgM/C5xdVU8ApwJfm+QoUyMqyUnAc4Ftk1S5CVjX3v8C8ABdfwN4EXB3Ve1O8pPAnwNvBH4EuA746yQH96zrNcAr6PrsTwIfAH6Trq8eBaycoql/BLwQ+DfAkcCbgO8PuN2p/ChwGLACOAd4X5IjquoSut/L/9n6+68MuL4FYxjMzsfaUcO+1+t6Z1bV7cANwJsHWNdNwC8l+dE2vblNrwaeDfx9kuOAFwNvrqpvt/V/EDirZz2fq6qPVdX36TrxLwC/X1VPVtWngamORM4BLquqG6rq+1W1s6q+MuB2p3NxVX2tqna3Nvz8DJbV4tLb7z82bt43knyL7lPx+4Hx8/f5HLAmyVF0IXApsCLJs3jqwc+/B65tffK7dH+8D6X7473PxVW1vaq+RXeg8vGq+nRVPUl3QPb9fg1on8T/A3Be6+vfq6rPtuUG2e5Uvgu8taq+W1XXAXuAnxpw2aEyDGbn9Ko6vOf1J33q/DfgPyVZPs269h0pvQT4NDBG90vxS8DftT/uxwK7q+qfe5b7Kt3Rxz7be94fCzzSjsB760/mOOAf+5QPst3pPNDz/pvAs2awrBaX3n5/+rh5R9P9355P15+f3m8F7Q/3rXT9+yV0/f+zdAcdvWFwLD19tv0ebGfqPr+9p/4TwMOT7MfRwCFM3uen2+5UHq6qvT3TI9PnDYN5UlVfAT4KTHnukq7z/1u6X6CbgM8w8Rfja8CReeqX0j8G7OzdZM/7XcAR7dx8b/3JbAf+VZ/y6bb7BPCMnnk/yuC8Xe4Bph1hvxv4NvD6KareBJxMd3r0C2365cAJdAdE0PW9H9+3QJLQHbRM1eeP66n/DLpTRf18o7Vxsj4/1Xa/yQHa5w2D+fWHwGvpzmn2VVX3AN8CfgO4qaoeBx4E/h0tDKpqO93R0/9oX8D+LN2pnb7XDVTVV+mOvv4wycFJfhGY6hzlpcBrk5yS5GlJViT56QG2eztwWpIj22muN077E/mBB4Gjkhw2g2U0Gt5J90XpIZPMv4nuVOOdVfUduk/D/xG4r6q+3upcDbyi9cmn033ieJKuP/azGXhlkl9s5/ffyiR/39rR/mXAu9sAiYOSvCjJDw2w3duBX2/LrKc7aBvUg8BPzKD+gjIMZuev89Tx1n/Vr1L7gvjDwDP7ze9xE93Hy+090wG+2FPnNcAquiOXvwIurKr/N8U6fx04kW6o64V0XxD3VVWfpwuti4DH2vb3HR1Ntd0PA39P92Xz3wJXTbOfvdv8Ct0Xdfe2c9COJjpwXAs8ArxukvmfpTsPv+9TwJ10R+r7pqmqu+kOkP6Y7kj+V+iGtn6n3wqraitwLt3AiV1t+1Ndx/K7wB10n0x2A+8CnjbAds9rZY/SjQ762BTbGO9S4PhJvnMZuvhwG0mSnwwkSYaBJMkwkCRhGEiSgJG9wdPRRx9dq1atGnYz5sUTTzzBM5853QCk0Tfs/bztttu+UVU/MrQGzNCo9vlh/z/P1qi2GyZv+1R9fmTDYNWqVdx6663Dbsa8GBsbY926dcNuxrwb9n4mmeqq7EVnVPv8sP+fZ2tU2w2Tt32qPu9pIkmSYaAlbVW71fA/7CtoV1PfkOSe9u8RrTxJLk53G+8vJ3lBzzIbW/17kmzsKX9hkjvaMhe3WxtMug1pmAwDLWXfoLu9dq8LgBurag1wY5uG7pbba9prE93tkmm3Kb+Q7mrvE4ALe/64f4DuKtx9y62fZhvS0BgGWsr20N2KoNcG4PL2/nLg9J7yK6pzM3B4kmPobrB2Q1XtrqpH6G5dvr7Ne3ZV3VzdZf5XjFtXv21IQ2MYSE+1vKp2tfcPAPtuQb6Cp94yeUcrm6p8R5/yqbYhDc3IjiaS5ltVVZJ5vXnXdNtoj0vcBLB8+XLGxsbmsznzYs+ePbZ7gc2m7YaB9FQPJjmmqna1Uz37nkm9k5775dM9UnFne60bVz7Wylf2qT/VNiZoj0u8BGDt2rU1ikMdR3WI5qi2G2bXdk8TSU+1hR88L3ojcE1P+VltVNFJwGPtVM/1wMuSHNG+OH4ZcH2b93iSk9ooorPGravfNqSh8ZOBlrLVdM/kPTrJDrpRQe8Erk5yDt3jD1/d6l4HnEb3oPdv0j3/gfbw9rfR3Rcfuuff7vtS+vXAh+ju3f+J9mKKbUhDYxgAqy64dlbL3f/OV8xxS7TA7quqtX3KTxlf0EYEndtvJVV1Gd2Ts8aX3wo8t0/5w/22MVv2X80FTxNJkgwDSZJhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIYMAyS/E6SrUn+IcmfJzkkyeoktyTZluSqJAe3uj/Upre1+at61vOWVn53kpf3lK9vZduS+HBwSVpg04ZBkhXAbwNrq+q5wEHAGcC7gIuq6jnAI8A5bZFzgEda+UWtHkmOb8v9DLAeeH+Sg5IcBLwPOBU4HnhNqytJWiCDniZaBhyaZBnwDGAXcDKwuc2/HDi9vd/QpmnzT2lPetoAXFlVT1bVfXQPCTmhvbZV1b1V9R3gylZXkrRApn24TVXtTPJHwD8B3wL+FrgNeLSq9rZqO4AV7f0KYHtbdm+Sx4CjWvnNPavuXWb7uPIT+7Vlvh4Ofv7z9k5fqY/5elj2KD+IeyaWyn5Ko2DaMGjPdd1A94jAR4G/oDvNs+Dm6+HgZ8/2SVFnzs32xxvlB3HPxFLZT2kUDHKa6KV0jwf8elV9F/go8GLg8HbaCGAlsLO93wkcB9DmHwY83Fs+bpnJyiVJC2SQMPgn4KQkz2jn/k8B7gQ+Bbyq1dkIXNPeb2nTtPmfbM+P3QKc0UYbrQbWAJ+ne5D4mjY66WC6L5m37P+uSZIGNch3Brck2Qx8EdgLfInuVM21wJVJ3t7KLm2LXAp8OMk2YDfdH3eqamuSq+mCZC9wblV9DyDJG4Dr6UYqXVZVW+duFyVJ05k2DACq6kLgwnHF99KNBBpf99vAr02ynncA7+hTfh1w3SBtkSTNPa9AliQN9slglKya5cggSVrK/GQgSTIMJEmGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoHUV5LfSbI1yT8k+fMkh7TbrN+SZFuSq9ot12m3Zb+qld+SZFXPet7Syu9O8vKe8vWtbFuSC4awi9JTGAbSOElWAL8NrK2q59LdWv0M4F3ARVX1HOAR4Jy2yDnAI638olaPJMe35X6G7umA709yUJKDgPcBpwLHA69pdaWhMQyk/pYBh7an9T0D2AWcDGxu8y8HTm/vN7Rp2vxT2oOgNgBXVtWTVXUfsI3utu8nANuq6t6q+g5wZasrDc0Bd9dSaX9V1c4kf0T3lL9vAX8L3AY8WlV7W7UdwIr2fgWwvS27N8ljwFGt/OaeVfcus31c+Yn92pJkE7AJYPny5YyNjU2oc/7z9k4oG0S/dc2HPXv2LNi25tKothtm13bDQBonyRF0R+qrgUeBv6A7zbPgquoSuicLsnbt2lq3bt2EOmfP8rbt9585cV3zYWxsjH7tXuxGtd0wu7Z7mkia6KXAfVX19ar6LvBR4MXA4e20EcBKYGd7vxM4DqDNPwx4uLd83DKTlUtDYxhIE/0TcFKSZ7Rz/6fQPbv7U8CrWp2NwDXt/ZY2TZv/yaqqVn5GG220GlgDfB74ArCmjU46mO5L5i0LsF/SpDxNJI1TVbck2Qx8EdgLfInuVM21wJVJ3t7KLm2LXAp8OMk2YDfdH3eqamuSq+mCZC9wblV9DyDJG4Dr6UYqXVZVWxdq/6R+DAOpj6q6ELhwXPG9dCOBxtf9NvBrk6znHcA7+pRfB1y3/y2V5oaniSRJhoEkyTCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiS8hbW0ZK2axeMy73/nK+ahJVoM/GQgSTIMJEmGgSSJAcMgyeFJNif5SpK7krwoyZFJbkhyT/v3iFY3SS5Osi3Jl5O8oGc9G1v9e5Js7Cl/YZI72jIXt4eQS5IWyKCfDN4L/E1V/TTwc8BdwAXAjVW1BrixTQOcCqxpr03ABwCSHEn3TNkT6Z4je+G+AGl1Xtez3Pr92y1J0kxMGwZJDgNeAlwKUFXfqapHgQ3A5a3a5cDp7f0G4Irq3AwcnuQY4OXADVW1u6oeAW4A1rd5z66qm6uqgCt61iVJWgCDDC1dDXwd+NMkPwfcBpwHLK+qXa3OA8Dy9n4FsL1n+R2tbKryHX3KJ0iyie7TBsuXL2dsbGxCnfOft3eAXZob/bY/F/bs2TNv615Mlsp+SqNgkDBYBrwA+K2quiXJe/nBKSEAqqqS1Hw0cNx2LgEuAVi7dm2tW7duQp2zZzF2erbuP3Pi9ufC2NgY/fbtQLNU9lMaBYN8Z7AD2FFVt7TpzXTh8GA7xUP796E2fydwXM/yK1vZVOUr+5RLkhbItGFQVQ8A25P8VCs6BbgT2ALsGxG0Ebimvd8CnNVGFZ0EPNZOJ10PvCzJEe2L45cB17d5jyc5qY0iOqtnXZKkBTDo7Sh+C/hIkoOBe4HX0gXJ1UnOAb4KvLrVvQ44DdgGfLPVpap2J3kb8IVW761Vtbu9fz3wIeBQ4BPtJUlaIAOFQVXdDqztM+uUPnULOHeS9VwGXNan/FbguYO0RZI097wCWZJkGEiSDANJEoaBJAnDQJKEYSBJwjCQ+vK27VpqDAOpP2/briXFMJDG8bbtWooGvR2FtJR42/ZJzOaW46N6q/JRbTfMru2GgTSRt22fxGxu2z6qtyof1XbD7NruaSJpIm/briXHMJDG8bbtWoo8TST1523btaQYBlIf3rZdS42niSRJhoEkyTCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEjMIgyQHJflSko+36dVJbkmyLclVSQ5u5T/Upre1+at61vGWVn53kpf3lK9vZduSXDCH+ydJGsBMPhmcB9zVM/0u4KKqeg7wCHBOKz8HeKSVX9TqkeR44AzgZ4D1wPtbwBwEvA84FTgeeE2rK0laIAOFQZKVwCuAD7bpACcDm1uVy4HT2/sNbZo2/5RWfwNwZVU9WVX3AduAE9prW1XdW1XfAa5sdSVJC2TZgPXeA7wJ+OE2fRTwaFXtbdM7gBXt/QpgO0BV7U3yWKu/Ari5Z529y2wfV35iv0Yk2QRsAli+fDljY2MT6pz/vL0TyuZLv+3PhT179szbuheTpbKf0iiYNgySvBJ4qKpuS7Ju3ls0haq6BLgEYO3atbVu3cTmnH3BtQvWnvvPnLj9uTA2Nka/fTvQLJX9lEbBIJ8MXgz8apLTgEOAZwPvBQ5Psqx9OlgJ7Gz1dwLHATuSLAMOAx7uKd+nd5nJyiVJC2Da7wyq6i1VtbKqVtF9AfzJqjoT+BTwqlZtI3BNe7+lTdPmf7KqqpWf0UYbrQbWAJ8HvgCsaaOTDm7b2DIneydJGsig3xn082bgyiRvB74EXNrKLwU+nGQbsJvujztVtTXJ1cCdwF7g3Kr6HkCSNwDXAwcBl1XV1v1olyRphmYUBlU1Boy19/fSjQQaX+fbwK9Nsvw7gHf0Kb8OuG4mbZEkzR2vQJYkGQbSZLzqXkuJYSBNzqvutWQYBlIfXnWvpWZ/RhNJB7L34FX3E8zmivFRvdJ8VNsNs2u7YSCN41X3k5vNVfejeqX5qLYbZtd2w0CayKvuteT4nYE0jlfdaynyk4E0OK+61wHLMJCm4FX3Wio8TSRJMgwkSYaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJLEAGGQ5Lgkn0pyZ5KtSc5r5UcmuSHJPe3fI1p5klycZFuSLyd5Qc+6Nrb69yTZ2FP+wiR3tGUuTpL52FlJUn+DfDLYC5xfVccDJwHnJjkeuAC4sarWADe2aYBTgTXttQn4AHThAVwInAicAFy4L0Bandf1LLd+/3dNkjSoacOgqnZV1Rfb+38G7gJWABuAy1u1y4HT2/sNwBXVuRk4PMkxwMuBG6pqd1U9AtwArG/znl1VN1dVAVf0rEuStACWzaRyklXA84FbgOVVtavNegBY3t6vALb3LLajlU1VvqNPeb/tb6L7tMHy5csZGxubUOf85+2dwR7tn37bnwt79uyZt3UvJktlP6VRMHAYJHkW8JfAG6vq8d7T+lVVSWoe2vcUVXUJcAnA2rVra926dRPqnH3BtfPdjH9x/5kTtz8XxsbG6LdvB5qlsp/SKBhoNFGSp9MFwUeq6qOt+MF2iof270OtfCdwXM/iK1vZVOUr+5RLQ+GgCS1Fg4wmCnApcFdVvbtn1hZgX+feCFzTU35W+wU5CXisnU66HnhZkiPaL9HLgOvbvMeTnNS2dVbPuqRhcNCElpxBPhm8GPhN4OQkt7fXacA7gV9Ocg/w0jYNcB1wL7AN+BPg9QBVtRt4G/CF9nprK6PV+WBb5h+BT8zBvkmz4qAJLUXTfmdQVZ8BJvsIe0qf+gWcO8m6LgMu61N+K/Dc6doiLbRhD5qQFsqMRhNJS8liGDRxIIygG9VRY6Pabphd2w0DqY+pBk1U1a4ZDJpYN658jBkMmjgQRtCN6qixUW03zK7t3ptIGsdBE1qK/GQgTbRv0MQdSW5vZb9HN0ji6iTnAF8FXt3mXQecRjcA4pvAa6EbNJFk36AJmDho4kPAoXQDJhw0oaEyDKRxHDShpcjTRJIkw0CSZBhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShA+32S+rZvHs2fvf+Yp5aIkk7R8/GUiSDANJkmEgScIwkCRhGEiSMAwkSRgGkiS8zkDSDMzm2poPrX/mPLREc81PBpIkw0CSZBhIkjAMJEkYBpIkDANJEoaBJAnDQJKEF50tuEEu2jn/eXs5u6eeD8SRNN8WzSeDJOuT3J1kW5ILht0eab7Z57WYLIowSHIQ8D7gVOB44DVJjh9uq6T5Y5/XYrNYThOdAGyrqnsBklwJbADuHGqrFgmftXxAWjJ9/o6djz3ltOcg7L8Lb7GEwQpge8/0DuDE8ZWSbAI2tck9Se5egLYtuN+Go4Fv7M868q45asz82u/93E8/PsRtL5k+P5v+vEj677D75/6YrO2T9vnFEgYDqapLgEuG3Y75luTWqlo77HbMt6Wyn/vjQOjzo/r/PKrthtm1fVF8ZwDsBI7rmV7ZyqQDlX1ei8piCYMvAGuSrE5yMHAGsGXIbZLmk31ei8qiOE1UVXuTvAG4HjgIuKyqtg65WcM00qcFZmCp7OcES6zPj+r/86i2G2bR9lTVfDREkjRCFstpIknSEBkGkiTDYNiSHJfkU0nuTLI1yXmt/MgkNyS5p/17xLDbur+SHJTkS0k+3qZXJ7ml3Y7hqvZFqkbUgdCXR7GPJjk8yeYkX0lyV5IXzeZnbhgM317g/Ko6HjgJOLfdluAC4MaqWgPc2KZH3XnAXT3T7wIuqqrnAI8A5wylVZorB0JfHsU++l7gb6rqp4Gfo2v/zH/mVeVrEb2Aa4BfBu4GjmllxwB3D7tt+7lfK1unPBn4OBC6KySXtfkvAq4fdjt9zen/+Uj15VHso8BhwH20wUA95TP+mfvJYBFJsgp4PnALsLyqdrVZDwDLh9WuOfIe4E3A99v0UcCjVbW3Te+gu0WDDgAj2pffw+j10dXA14E/bae3PpjkmcziZ24YLBJJngX8JfDGqnq8d1518T6yY4CTvBJ4qKpuG3ZbNP9GsS+PcB9dBrwA+EBVPR94gnGnhAb9mS+Ki86WuiRPp/vl+UhVfbQVP5jkmKraleQY4KHhtXC/vRj41SSnAYcAz6Y7z3l4kmXtyMvbMRwARrgvj2of3QHsqKpb2vRmujCY8c/cTwZDliTApcBdVfXunllbgI3t/Ua6868jqareUlUrq2oV3W0XPllVZwKfAl7Vqo30Pmq0+/Ko9tGqegDYnuSnWtEpdLdBn/HP3CuQhyzJLwJ/B9zBD85V/h7dudargR8Dvgq8uqp2D6WRcyjJOuB3q+qVSX4CuBI4EvgS8BtV9eQQm6f9cKD05VHro0l+HvggcDBwL/BaugP9Gf3MDQNJkqeJJEmGgSQJw0CShGEgScIwkCRhGEiSMAwkScD/B9M0kkXk3x6VAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[\"EN word count\"] = df[\"EN\"].apply(lambda x: len(prepare_sentence(x).split(\" \")))\n",
    "df[\"FR word count\"] = df[\"FR\"].apply(lambda x: len(prepare_sentence(x).split(\" \")))\n",
    "df.hist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the distributions, in both languages it would be okay to select $20$ as the max size of an input/output sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffling and generating a subsample of the dataframe.\n",
    "DATASET_FRACTION = 0.5\n",
    "df = df.sample(frac=DATASET_FRACTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You should first download these two spaCy models!\n",
    "en_tokenizer = get_tokenizer(\"spacy\", \"en_core_web_sm\")\n",
    "fr_tokenizer = get_tokenizer(\"spacy\", \"fr_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_corpus(corpus: List[str], tokenizer: spacy.tokenizer.Tokenizer, max_len: int):\n",
    "    for sentence in corpus:\n",
    "        tokens = tokenizer(\n",
    "            prepare_sentence(sentence)\n",
    "        )\n",
    "\n",
    "        # Adding padding if it is needed.\n",
    "        if len(tokens) >= max_len:\n",
    "            tokens = tokens[:max_len]\n",
    "        else:\n",
    "            len_diff = max_len - len(tokens)\n",
    "            tokens = tokens + [\"<pad>\"] * len_diff\n",
    "\n",
    "        yield tokens\n",
    "\n",
    "\n",
    "en_corpus = [sent for sent in list(df[\"EN\"])]\n",
    "fr_corpus = [sent for sent in list(df[\"FR\"])]\n",
    "EN_MAX_LEN = 20\n",
    "FR_MAX_LEN = 20\n",
    "SPECIALS = [\"<unk>\", \"<start>\", \"<end>\", \"<pad>\"]\n",
    "\n",
    "en_vocab = build_vocab_from_iterator(\n",
    "    iterate_corpus(en_corpus, en_tokenizer, EN_MAX_LEN), \n",
    "    specials=SPECIALS\n",
    ")\n",
    "en_vocab.set_default_index(en_vocab[\"<unk>\"])\n",
    "\n",
    "fr_vocab = build_vocab_from_iterator(\n",
    "    iterate_corpus(fr_corpus, fr_tokenizer, FR_MAX_LEN), \n",
    "    specials=SPECIALS\n",
    ")\n",
    "fr_vocab.set_default_index(fr_vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the 'l1_idx2token' attribute:\n",
      "['<start>', 'I', \"'m\", 'not', 'that', 'cynical', '.', '<end>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "x.shape: torch.Size([87810, 22])\n",
      "y.shape: torch.Size([87810, 22])\n"
     ]
    }
   ],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        lang1_corpus: List[str], lang2_corpus: List[str],\n",
    "        lang1_tokenizer: spacy.tokenizer.Tokenizer, lang2_tokenizer: spacy.tokenizer.Tokenizer,\n",
    "        lang1_vocab: torchtext.vocab.Vocab, lang2_vocab: torchtext.vocab.Vocab,\n",
    "        lang1_max_len: int = 200, lang2_max_len: int = 200, special_tokens=SPECIALS\n",
    "    ):\n",
    "        self.l1_corpus = lang1_corpus\n",
    "        self.l2_corpus = lang2_corpus\n",
    "\n",
    "        self.l1_max_len = lang1_max_len\n",
    "        self.l2_max_len = lang2_max_len\n",
    "        \n",
    "        self.l1_tokenizer = lang1_tokenizer\n",
    "        self.l2_tokenizer = lang2_tokenizer\n",
    "        \n",
    "        self.l1_vocab = lang1_vocab\n",
    "        self.l2_vocab = lang2_vocab\n",
    "\n",
    "        self.l1_token2idx = lang1_vocab\n",
    "        self.l1_idx2token = {\n",
    "            lang1_vocab[word]: word\n",
    "            for sentence in lang1_corpus\n",
    "            for word in lang1_tokenizer(prepare_sentence(sentence)) + SPECIALS\n",
    "        }\n",
    "        self.l2_token2idx = lang2_vocab\n",
    "        self.l2_idx2token = {\n",
    "            lang2_vocab[word]: word\n",
    "            for sentence in lang2_corpus\n",
    "            for word in lang2_tokenizer(prepare_sentence(sentence)) + SPECIALS\n",
    "        }\n",
    "\n",
    "        self.x, self.y = self._get_x_y()\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def _get_x_y(self):\n",
    "        x = TranslationDataset._parse_corpus(\n",
    "            self.l1_corpus, self.l1_tokenizer, self.l1_vocab, self.l1_max_len\n",
    "        )\n",
    "        y = TranslationDataset._parse_corpus(\n",
    "            self.l2_corpus, self.l2_tokenizer, self.l2_vocab, self.l2_max_len\n",
    "        )\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    @staticmethod\n",
    "    def _parse_corpus(\n",
    "        corpus: List[str], \n",
    "        tokenizer: spacy.tokenizer.Tokenizer, \n",
    "        vocab: torchtext.vocab.Vocab,\n",
    "        max_len: int\n",
    "    ):\n",
    "        output = []\n",
    "\n",
    "        for sent in corpus:\n",
    "            tokens = tokenizer(sent)\n",
    "            indices = [vocab[token] for token in tokens]\n",
    "            if len(indices) >= max_len:\n",
    "                output.append([vocab[\"<start>\"]] + indices[:max_len] + [vocab[\"<end>\"]])\n",
    "            else:\n",
    "                len_diff = max_len - len(indices)\n",
    "                padding = [vocab[\"<pad>\"]] * len_diff\n",
    "                output.append([vocab[\"<start>\"]] + indices + [vocab[\"<end>\"]] + padding)\n",
    "\n",
    "        return torch.LongTensor(output)\n",
    "\n",
    "\n",
    "dataset = TranslationDataset(\n",
    "    lang1_corpus=en_corpus, lang2_corpus=fr_corpus,\n",
    "    lang1_vocab=en_vocab, lang2_vocab=fr_vocab,\n",
    "    lang1_tokenizer=en_tokenizer, lang2_tokenizer=fr_tokenizer,\n",
    "    lang1_max_len=EN_MAX_LEN, lang2_max_len=FR_MAX_LEN\n",
    ")\n",
    "# Printing an example sentence, using the l1_idx2token dictionary.\n",
    "# l1 here means language 1, which is English.\n",
    "x_sample = dataset.x[1]\n",
    "print(\"Using the 'l1_idx2token' attribute:\")\n",
    "print([dataset.l1_idx2token[int(idx)] for idx in x_sample])\n",
    "\n",
    "print(\"x.shape:\", dataset.x.shape)\n",
    "print(\"y.shape:\", dataset.y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 70248\n",
      "Validation dataset size: 17562\n"
     ]
    }
   ],
   "source": [
    "def train_validation_split(dataset: torch.utils.data.Dataset, train_size: float):\n",
    "    train_set_size = int(len(dataset) * train_size)\n",
    "    valid_set_size = len(dataset) - train_set_size\n",
    "    datasets_lengths = [train_set_size, valid_set_size]\n",
    "\n",
    "    # Splitting the input dataset into training and validation set.\n",
    "    train_dataset, valid_dataset = random_split(dataset, datasets_lengths)\n",
    "\n",
    "    return train_dataset, valid_dataset\n",
    "\n",
    "\n",
    "train_dataset, valid_dataset = train_validation_split(\n",
    "    dataset, train_size=0.8\n",
    ")\n",
    "print(\"Training dataset size:\", len(train_dataset))\n",
    "print(\"Validation dataset size:\", len(valid_dataset))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Seq2Seq Model\n",
    "The model is constructed by two *LSTM*s - an *Encoder* and a *Decoder*.\n",
    "- *Encoder* - takes the input sequence, projects it onto a certain space (embeds it), passes it through *LSTM* cells and returns the output and most importantly the *hidden state* that's going to be passed to the *Decoder*.\n",
    "- *Decoder* - uses the *hidden state* from the *Encoder* and takes the target sequence (the expected output). On inference we only pass the '*\\<start\\>*' token and the model can be used by making 1-by-1 predictions. As an architecture it is almost the same as the *Encoder*, except for its last layer, which projects the output to $\\mathbb{R}^{vocab}$ space - i.e. a vector of the size of the vocabulary (in our case it's the vocabulary of the French language).\n",
    "\n",
    "An interesting thing that has been added by the creators of the paper is the reversal of the input sequence before it's passed to the *Encoder*:\n",
    "> ... we found\n",
    "> it extremely valuable to reverse the order of the words of the input sentence. So for example, instead\n",
    "> of mapping the sentence a, b, c to the sentence α, β, γ, the LSTM is asked to map c, b, a to α, β, γ,\n",
    "> where α, β, γ is the translation of a, b, c. This way, a is in close proximity to α, b is fairly close to β,\n",
    "> and so on, a fact that makes it easy for SGD to “establish communication” between the input and the\n",
    "> output. \n",
    "\n",
    "This may be seen in the `Seq2Seq` class, in `forward()`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: torch.Size([10, 5, 100])\n"
     ]
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size: int, embed_size: int, hidden_size: int, num_layers: int, padding_idx: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size, padding_idx=padding_idx)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embed_size, \n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, hidden: tuple):\n",
    "        embedded_x = self.embed(x)\n",
    "        hidden = [state.detach() for state in hidden]\n",
    "        out, hidden = self.lstm(embedded_x, hidden)\n",
    "\n",
    "        return out, hidden\n",
    "\n",
    "    def get_init_hidden(self, batch_size):\n",
    "        # See the PyTorch documentation of LSTM for these dimensions.\n",
    "        h = torch.zeros(size=(self.num_layers, batch_size, self.hidden_size))\n",
    "        c = torch.zeros(size=(self.num_layers, batch_size, self.hidden_size))\n",
    "\n",
    "        return (h, c)\n",
    "\n",
    "\n",
    "x = torch.randint(low=0, high=20, size=(10, 5))\n",
    "enc = Encoder(vocab_size=50, embed_size=100, hidden_size=100, num_layers=4, padding_idx=0)\n",
    "h, c = enc.get_init_hidden(10)\n",
    "out, hidden = enc(x, (h, c))\n",
    "print(f\"Encoder output shape: {out.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: torch.Size([10, 5, 150])\n",
      "Decoder output shape: torch.Size([10, 5, 200])\n"
     ]
    }
   ],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size: int, embed_size: int, hidden_size: int, num_layers: int, padding_idx: int):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size, padding_idx=padding_idx)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embed_size, \n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.project = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, hidden: tuple):\n",
    "        embedded_x = self.embed(x)\n",
    "        hidden = [state.detach() for state in hidden]\n",
    "        out, hidden = self.lstm(embedded_x, hidden)\n",
    "        out = self.project(out)\n",
    "\n",
    "        return out, hidden\n",
    "\n",
    "\n",
    "# Passing through the Encoder\n",
    "x = torch.randint(low=0, high=20, size=(10, 5))\n",
    "enc = Encoder(vocab_size=50, embed_size=100, hidden_size=150, num_layers=4, padding_idx=0)\n",
    "hidden = enc.get_init_hidden(10)\n",
    "out, hidden = enc(x, hidden)\n",
    "print(f\"Encoder output shape: {out.shape}\")\n",
    "y = torch.randint(low=0, high=40, size=(10, 5))\n",
    "\n",
    "# Passing through the Decoder\n",
    "dec = Decoder(vocab_size=200, embed_size=120, hidden_size=150, num_layers=4, padding_idx=0)\n",
    "out, hidden = dec(y, hidden)\n",
    "print(f\"Decoder output shape: {out.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embed): Embedding(50, 100, padding_idx=0)\n",
      "    (lstm): LSTM(100, 150, num_layers=4, batch_first=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embed): Embedding(100, 100, padding_idx=0)\n",
      "    (lstm): LSTM(100, 150, num_layers=4, batch_first=True)\n",
      "    (project): Linear(in_features=150, out_features=100, bias=True)\n",
      "  )\n",
      ")\n",
      "x shape: torch.Size([10, 5])\n",
      "y shape: torch.Size([10, 5])\n",
      "Seq2Seq output shape: torch.Size([10, 5, 100])\n",
      "Seq2Seq hidden shape: torch.Size([4, 10, 150])\n"
     ]
    }
   ],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder: torch.nn.Module, decoder: torch.nn.Module, reverse_input=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "        self.reverse_input = reverse_input\n",
    "\n",
    "    def forward(self, x: torch.Tensor, y: torch.Tensor, hidden: tuple):\n",
    "        # Reversing the sequences in the input tensor.\n",
    "        # In the paper it's stated that it is extremely valuable, and makes a difference.\n",
    "        if self.reverse_input:\n",
    "            x = torch.flip(x, [1])\n",
    "        _, hidden = self.encoder(x, hidden)\n",
    "        out, hidden = self.decoder(y, hidden)\n",
    "\n",
    "        return out, hidden\n",
    "\n",
    "\n",
    "# Input tensor:\n",
    "x = torch.randint(low=0, high=20, size=(10, 5))\n",
    "y = torch.randint(low=0, high=40, size=(10, 5))\n",
    "\n",
    "# Defining the Encoder:\n",
    "enc = Encoder(vocab_size=50, embed_size=100, hidden_size=150, num_layers=4, padding_idx=0)\n",
    "init_hidden = enc.get_init_hidden(10)\n",
    "\n",
    "# Defining the decoder:\n",
    "dec = Decoder(vocab_size=100, embed_size=100, hidden_size=150, num_layers=4, padding_idx=0)\n",
    "\n",
    "# Creating the Seq2Seq model:\n",
    "model = Seq2Seq(encoder=enc, decoder=dec)\n",
    "out, hidden = model(x, y, init_hidden)\n",
    "\n",
    "print(f\"Model:\\n{model}\")\n",
    "\n",
    "print(f\"x shape: {x.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "\n",
    "print(f\"Seq2Seq output shape: {out.shape}\")\n",
    "print(f\"Seq2Seq hidden shape: {hidden[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(model: torch.nn.Module):\n",
    "    \"\"\"Initializing the weights with the uniform distribution between -0.08 and \n",
    "    0.08.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model for which the weights will be initialized.\n",
    "    \"\"\"\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"bias\" in name or \"weight\" in name:\n",
    "            torch.nn.init.uniform_(param, a=-0.08, b=0.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model: torch.nn.Module, path: str):\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "\n",
    "def load_checkpoint(model: torch.nn.Module, path: str):\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    return model\n",
    "\n",
    "\n",
    "MODEL_FILENAME = \"1-seq2seq.pt\"\n",
    "MODEL_PATH = os.path.join(\"..\", \"models\", MODEL_FILENAME)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training\n",
    "As for the training there are some differences between the paper and this implementation but they're insignificant.\n",
    "\n",
    "In the paper they're using a *uniform distribution* for the weight initialization:\n",
    "> We initialized all of the LSTM’s parameters with the uniform distribution between -0.08\n",
    "> and 0.08\n",
    "\n",
    "That's done in `initialize_weights()` above.\n",
    "\n",
    "\n",
    "> We used stochastic gradient descent without momentum, with a fixed learning rate of 0.7.\n",
    "> After 5 epochs, we begun halving the learning rate every half epoch. We trained our models\n",
    "> for a total of 7.5 epochs.\n",
    "\n",
    "I am using the SGD optimizer with learning rate of $0.7$ but I am not covering the halving of it on every half epoch.\n",
    "\n",
    "> We used batches of 128 sequences\n",
    "\n",
    "Here, I've used batch size of $64$ since my VRAM is not that up to par.\n",
    "\n",
    "> Although LSTMs tend to not suffer from the vanishing gradient problem, they can have\n",
    "> exploding gradients. Thus we enforced a hard constraint on the norm of the gradient [10,\n",
    "> 25] by scaling it when its norm exceeded a threshold.\n",
    "\n",
    "I am also clipping the gradients based on the norm - see `Seq2SeqTrainingSession._train_epoch()`. I've used the `torch.nn.utils.clip_grad_norm_()` method.\n",
    "\n",
    "The model is also trained for $5$ epochs as it is stated in the paper.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Training Batch 0, Loss: 9.89\n",
      "Bleu Score: 0.00\n",
      "Example\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Target: <start> Tom t' aurait vraiment aimé . <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Prediction: squatter tableau tableau tableau tableau tableau tableau tableau tableau tableau tableau tableau tableau tableau tableau tableau tableau tableau tableau tableau tableau tableau\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training Batch 150, Loss: 2.92\n",
      "Bleu Score: 0.00\n",
      "Example\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Target: <start> Il essaya de rattraper le temps perdu . <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Prediction: <start> <start> Je est . . <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training Batch 300, Loss: 2.53\n",
      "Bleu Score: 0.00\n",
      "Example\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Target: <start> Les fourmis ont - elles des oreilles ? <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Prediction: <start> Je ne est pas . . . . . <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training Batch 450, Loss: 2.24\n",
      "Bleu Score: 0.00\n",
      "Example\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Target: <start> Tom est là - dedans tout seul . <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Prediction: <start> Je est est à à la la . <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training Batch 600, Loss: 2.31\n",
      "Bleu Score: 0.00\n",
      "Example\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Target: <start> Les jeunes enfants sont très curieux . <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Prediction: <start> Je n' que pas de de . <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training Batch 750, Loss: 1.99\n",
      "Bleu Score: 0.00\n",
      "Example\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Target: <start> Nous allons essuyer une tempête . <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Prediction: <start> Je est un de de . <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training Batch 900, Loss: 1.94\n",
      "Bleu Score: 0.00\n",
      "Example\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Target: <start> Si tu te mouilles les pieds , tu attraperas froid . <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Prediction: <start> Je est a le de pour que je je le . <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training Batch 1050, Loss: 1.82\n",
      "Bleu Score: 0.00\n",
      "Example\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Target: <start> Je ne vois pas comment tu peux ignorer ça . <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Prediction: <start> Je ne suis pas en pas il ce en . <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 1, Training Loss: 2.12, Train BLEU: 0.00, Validation Loss: 1.96, Validation BLEU: 0.00\n",
      "Training Batch 0, Loss: 1.99\n",
      "Bleu Score: 0.00\n",
      "Example\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Target: <start> Il y a une chose que je ne comprends pas . <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Prediction: <start> Je est que en pas que que que je pas . <end> <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training Batch 150, Loss: 1.77\n",
      "Bleu Score: 0.00\n",
      "Example\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Target: <start> Ne soyez pas si gourmand ou vous finirez sans rien . <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Prediction: <start> Tom -ce que tu chose ce , il chose chose . <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training Batch 300, Loss: 1.69\n",
      "Bleu Score: 0.00\n",
      "Example\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Target: <start> Je continue de dire que je suis innocent , mais personne ne m' écoute . <end> <pad> <pad> <pad> <pad> <pad>\n",
      "Prediction: <start> Je pense de mon que que je jamais pas il ce que je j' . <end> <pad> <pad> <pad> <pad> <pad>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training Batch 450, Loss: 1.56\n",
      "Bleu Score: 0.00\n",
      "Example\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Target: <start> C' est le meilleur moyen . <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Prediction: <start> C' est la son avec . <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training Batch 600, Loss: 1.53\n",
      "Bleu Score: 0.00\n",
      "Example\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Target: <start> Tu me le donnas . <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Prediction: <start> Tom ai le peu . <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training Batch 750, Loss: 1.63\n",
      "Bleu Score: 0.00\n",
      "Example\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Target: <start> Mon tableau commence à avoir de la gueule . <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Prediction: <start> Ils se était la voiture de la maison . <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training Batch 900, Loss: 1.42\n",
      "Bleu Score: 0.00\n",
      "Example\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Target: <start> Le match n' a pas eu lieu . <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Prediction: <start> C' -moi ne tu pas -ce chose . <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training Batch 1050, Loss: 1.25\n",
      "Bleu Score: 0.00\n",
      "Example\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Target: <start> Tom n' était pas aussi rapide que moi . <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Prediction: <start> Tom ne peux pas besoin chose que ça . <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 2, Training Loss: 1.50, Train BLEU: 0.00, Validation Loss: 1.49, Validation BLEU: 0.00\n",
      "Training Batch 0, Loss: 1.47\n",
      "Bleu Score: 0.00\n",
      "Example\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Target: <start> J' ai eu le même âge que vous . <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Prediction: <start> J' ai jamais à monde chose que vous . <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training Batch 150, Loss: 1.45\n",
      "Bleu Score: 0.00\n",
      "Example\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Target: <start> Ne me dis pas que tu l' as volée ! <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Prediction: <start> Tom a besoin pas que tu l' ce elle ? <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training Batch 300, Loss: 1.27\n",
      "Bleu Score: 0.00\n",
      "Example\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Target: <start> Je veux que vous alliez dans votre chambre et que vous emballiez vos affaires . <end> <pad> <pad> <pad> <pad> <pad>\n",
      "Prediction: <start> Je veux que vous soit en faire travail et que vous soit ici chose . <end> <pad> <pad> <pad> <pad> <pad>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training Batch 450, Loss: 1.37\n",
      "Bleu Score: 0.00\n",
      "Example\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Target: <start> Ça n' a pas été un problème . <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Prediction: <start> Ce n' est pas dit le peu . <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training Batch 600, Loss: 1.41\n",
      "Bleu Score: 0.00\n",
      "Example\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Target: <start> Avez -vous été obéissante ? <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Prediction: <start> Les -vous tout chose ? <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training Batch 750, Loss: 1.28\n",
      "Bleu Score: 0.00\n",
      "Example\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Target: <start> Je veux être choyée . <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Prediction: <start> Je veux être air . <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training Batch 900, Loss: 1.37\n",
      "Bleu Score: 0.00\n",
      "Example\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Target: <start> Aussitôt que Tom arrivera ici , nous partirons . <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Prediction: <start> C' que tu besoin ici , tu soit . <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training Batch 1050, Loss: 1.29\n",
      "Bleu Score: 0.00\n",
      "Example\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Target: <start> De quoi as - tu si peur ? <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Prediction: <start> C' -moi était il il fait soit ? <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 3, Training Loss: 3.28, Train BLEU: 0.00, Validation Loss: 1.87, Validation BLEU: 0.00\n",
      "Training Batch 0, Loss: 2.22\n",
      "Bleu Score: 0.00\n",
      "Example\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Target: <start> Nous parlions de tout et de rien . <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Prediction: <start> Tom es . la Tom de la <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training Batch 150, Loss: 1.36\n",
      "Bleu Score: 0.00\n",
      "Example\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Target: <start> Je ne sais pas ce que nous pouvons faire d' autre . <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Prediction: <start> Je ne sais pas ce que j' soit en Tom être . <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training Batch 300, Loss: 1.23\n",
      "Bleu Score: 0.00\n",
      "Example\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Target: <start> Ces clés ne sont pas à moi . <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Prediction: <start> Les -moi que sont , la plus . <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training Batch 450, Loss: 1.33\n",
      "Bleu Score: 0.00\n",
      "Example\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Target: <start> Je veux être seul un moment . <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Prediction: <start> Je suis eu quelque en peu . <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training Batch 600, Loss: 1.49\n",
      "Bleu Score: 0.00\n",
      "Example\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Target: <start> Je suis sûre de remporter le match de tennis . <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Prediction: <start> Je suis désolé de maison de maison de maison . <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training Batch 750, Loss: 1.18\n",
      "Bleu Score: 0.00\n",
      "Example\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Target: <start> Je veux que tu analyses ceci . <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Prediction: <start> Je veux que Tom sois quelque . <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training Batch 900, Loss: 1.26\n",
      "Bleu Score: 0.00\n",
      "Example\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Target: <start> Je détestais ce film . <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Prediction: <start> Je sais ce chose . <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training Batch 1050, Loss: 1.19\n",
      "Bleu Score: 0.00\n",
      "Example\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Target: <start> Tom est maintenant conscient . <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Prediction: <start> Tu a eu besoin . <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 4, Training Loss: 1.31, Train BLEU: 0.00, Validation Loss: 1.19, Validation BLEU: 0.00\n",
      "Training Batch 0, Loss: 1.06\n",
      "Bleu Score: 0.00\n",
      "Example\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Target: <start> J' aime ce boulot . <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Prediction: <start> J' aime qu' parler . <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training Batch 150, Loss: 1.07\n",
      "Bleu Score: 0.00\n",
      "Example\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Target: <start> Ma prémonition s' avéra . <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Prediction: <start> Les père s' ont . <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training Batch 300, Loss: 1.18\n",
      "Bleu Score: 0.00\n",
      "Example\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Target: <start> C' est de votre faute . <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Prediction: <start> C' est de mon argent . <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training Batch 450, Loss: 1.14\n",
      "Bleu Score: 0.00\n",
      "Example\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Target: <start> Ce garçon porte plein de promesses . <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Prediction: <start> Ils ont -je -il de tête . <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training Batch 600, Loss: 1.15\n",
      "Bleu Score: 0.00\n",
      "Example\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Target: <start> Tu es l' enseignant . <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Prediction: <start> Tu es un intention . <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training Batch 750, Loss: 1.24\n",
      "Bleu Score: 0.00\n",
      "Example\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Target: <start> Combien de gens compte votre équipage ? <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Prediction: <start> Le de monde chose du chambre ? <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training Batch 900, Loss: 1.11\n",
      "Bleu Score: 0.00\n",
      "Example\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Target: <start> Par chance , il a gagné le championnat . <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Prediction: <start> À quelle , nous a demandé le livre . <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training Batch 1050, Loss: 0.99\n",
      "Bleu Score: 0.00\n",
      "Example\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Target: <start> Je dois partir tôt pour attraper le train . <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Prediction: <start> Je pense prendre quelques pour choses le voiture . <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 5, Training Loss: 1.00, Train BLEU: 0.00, Validation Loss: 1.17, Validation BLEU: 0.00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embed): Embedding(12515, 300, padding_idx=3)\n",
       "    (lstm): LSTM(300, 512, num_layers=4, batch_first=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embed): Embedding(20099, 300, padding_idx=3)\n",
       "    (lstm): LSTM(300, 512, num_layers=4, batch_first=True)\n",
       "    (project): Linear(in_features=512, out_features=20099, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Seq2SeqTrainingSession:\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        model: torch.nn.Module, \n",
    "        loss: torch.nn.Module, \n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        epochs: int, batch_size: int, \n",
    "        use_clipping=True,\n",
    "        device=\"cpu\"\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.loss_func = loss\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.use_clipping = use_clipping\n",
    "        self.clip = 5\n",
    "        self.device = device\n",
    "\n",
    "    def start(\n",
    "        self, \n",
    "        train_dataset: torch.utils.data.Subset, \n",
    "        valid_dataset: torch.utils.data.Subset,\n",
    "        initial_hidden: torch.Tensor,\n",
    "        l1_idx2token: dict,\n",
    "        l2_idx2token: dict,\n",
    "        fixed_sentences: List[List[str]] = None\n",
    "    ):\n",
    "        self.init_hidden = initial_hidden\n",
    "        self.fixed_sentences = fixed_sentences\n",
    "\n",
    "        self.l1_idx2token = l1_idx2token\n",
    "        self.l2_idx2token = l2_idx2token\n",
    "        \n",
    "        train_dataloader = DataLoader(\n",
    "            train_dataset, \n",
    "            self.batch_size, \n",
    "            shuffle=True, \n",
    "            num_workers=0,\n",
    "            drop_last=True\n",
    "        )\n",
    "        valid_dataloader = DataLoader(\n",
    "            train_dataset, \n",
    "            self.batch_size, \n",
    "            shuffle=True, \n",
    "            num_workers=0,\n",
    "            drop_last=True\n",
    "        )\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            train_loss, train_bleu = self._train_epoch(train_dataloader)\n",
    "            valid_loss, valid_bleu = self._valid_epoch(valid_dataloader)\n",
    "            print(f\"Epoch: {epoch + 1}, Training Loss: {train_loss:.2f}, Train BLEU: {train_bleu:.2f}, Validation Loss: {valid_loss:.2f}, Validation BLEU: {valid_bleu:.2f}\")\n",
    "\n",
    "        return self.model\n",
    "\n",
    "    def _train_epoch(self, dataloader):\n",
    "        hidden = self.init_hidden\n",
    "\n",
    "        for batch_i, (x, y) in enumerate(dataloader):\n",
    "            x, y = x.to(self.device), y.to(self.device)\n",
    "\n",
    "            y_pred, hidden = self.model(x, y, hidden)\n",
    "\n",
    "            # Here y_pred should be trasformed to shape (BATCH_SIZE * SEQ_LEN, VOCAB_SIZE)\n",
    "            # and y should be transformed to shape (BATCH_SIZE * SEQ_LEN).\n",
    "            loss = self.loss_func(y_pred.reshape((-1, y_pred.shape[-1])), y.reshape(-1))\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            # Clipping the gradients, since LSTMs can have exploding gradients.\n",
    "            if self.use_clipping:\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    self.model.parameters(), \n",
    "                    max_norm=self.clip\n",
    "                )\n",
    "\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if batch_i % 150 == 0:\n",
    "                pred_sents, target_sents = self._tensors2sentences(y_pred[:1], y[:1])\n",
    "                print(f\"Training Batch {batch_i}, Loss: {loss.item():.2f}\")\n",
    "                bleu = self._calc_bleu(y_pred, y)\n",
    "                print(f\"Bleu Score: {bleu:.2f}\")\n",
    "                print(\"Example\")\n",
    "                print(\"-\" * 100)\n",
    "                print(f\"Target: {' '.join(target_sents)}\")\n",
    "                print(f\"Prediction: {' '.join(pred_sents)}\")\n",
    "                print(\"-\" * 100)\n",
    "\n",
    "                # Saving the model checkpoint.\n",
    "                save_checkpoint(model=self.model, path=MODEL_PATH)\n",
    "\n",
    "        epoch_loss = loss.item()\n",
    "        epoch_bleu = self._calc_bleu(y_pred, y)\n",
    "\n",
    "        return epoch_loss, epoch_bleu\n",
    "\n",
    "    def _valid_epoch(self, dataloader):\n",
    "        hidden = self.init_hidden\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_i, (x, y) in enumerate(dataloader):\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "            \n",
    "                y_pred, hidden = self.model(x, y, hidden)\n",
    "\n",
    "                loss = self.loss_func(y_pred.reshape((-1, y_pred.shape[-1])), y.reshape(-1))\n",
    "\n",
    "            epoch_loss = loss.item()\n",
    "            epoch_bleu = self._calc_bleu(y_pred, y)\n",
    "\n",
    "        self.model.train()\n",
    "        \n",
    "        return epoch_loss, epoch_bleu\n",
    "\n",
    "    def _tensors2sentences(self, y_pred: torch.Tensor, y):\n",
    "        predictions = y_pred.argmax(-1).tolist()\n",
    "        targets = y.tolist()\n",
    "        predictions = [\n",
    "            self.l2_idx2token[idx] \n",
    "            for sent in predictions \n",
    "            for idx in sent\n",
    "        ]\n",
    "        targets = [\n",
    "            self.l2_idx2token[idx] \n",
    "            for sent in targets \n",
    "            for idx in sent\n",
    "        ]\n",
    "\n",
    "        return predictions, targets\n",
    "\n",
    "    def _calc_bleu(self, y_pred, y):\n",
    "        predictions = y_pred.argmax(-1).tolist()\n",
    "        targets = y.tolist()\n",
    "\n",
    "        prediction_words = []\n",
    "        target_words = []\n",
    "        for i, (pred_sent, targ_sent) in enumerate(zip(predictions, targets)):\n",
    "            prediction_words.append([])\n",
    "            target_words.append([])\n",
    "            for pred_idx, targ_idx in zip(pred_sent, targ_sent):\n",
    "                prediction_words[i].append(self.l2_idx2token[pred_idx])\n",
    "                target_words[i].append(self.l2_idx2token[targ_idx])\n",
    "\n",
    "        return bleu_score(prediction_words, target_words)\n",
    "\n",
    "\n",
    "# The paper uses 1000, but their dataset is huge.\n",
    "EMBED_SIZE = 300\n",
    "HIDDEN_SIZE = 512\n",
    "NUM_LAYERS = 4\n",
    "# The batch size was 128 in the paper, but my GPU has 2GB VRAM :[.\n",
    "BATCH_SIZE = 64\n",
    "L_RATE = 7e-1\n",
    "# The epochs in the paper are 7.5 (7 epochs and a half).\n",
    "# The model and the dataset are somewhat different, so I'll use different\n",
    "# number of epochs.\n",
    "EPOCHS = 5\n",
    "\n",
    "LOAD_MODEL = False\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "enc = Encoder(\n",
    "    vocab_size=len(en_vocab),\n",
    "    embed_size=EMBED_SIZE,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    padding_idx=en_vocab[\"<pad>\"]\n",
    ")\n",
    "init_hidden = enc.get_init_hidden(batch_size=BATCH_SIZE)\n",
    "init_hidden = [h.to(DEVICE) for h in init_hidden]\n",
    "\n",
    "dec = Decoder(\n",
    "    vocab_size=len(fr_vocab),\n",
    "    embed_size=EMBED_SIZE,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    padding_idx=fr_vocab[\"<pad>\"]\n",
    ")\n",
    "\n",
    "model = Seq2Seq(\n",
    "    encoder=enc, decoder=dec, \n",
    "    reverse_input=True\n",
    ").to(DEVICE)\n",
    "\n",
    "if LOAD_MODEL:\n",
    "    model = load_checkpoint(model, path=MODEL_PATH)\n",
    "else:\n",
    "    model.apply(initialize_weights)\n",
    "\n",
    "# The loss function and the optimizer.\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=L_RATE)\n",
    "\n",
    "training_session = Seq2SeqTrainingSession(\n",
    "    model=model,\n",
    "    loss=loss,\n",
    "    optimizer=optimizer,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    use_clipping=True,\n",
    "    device=DEVICE\n",
    ")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "training_session.start(\n",
    "    train_dataset=train_dataset, \n",
    "    valid_dataset=valid_dataset, \n",
    "    initial_hidden=init_hidden,\n",
    "    l1_idx2token=dataset.l1_idx2token,\n",
    "    l2_idx2token=dataset.l2_idx2token\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8100bb27ef6f27bb6b63ba202e13f32f0dffed430e6a4d162d3986e448f218b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
